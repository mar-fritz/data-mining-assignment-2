{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Εργασία 2 (Τεχνικές Εξόρυξης Δεδομένων)\n",
    "## Data Mining: Assignment 2\n",
    "***\n",
    "### Μαρία Φριτζελά 1115201400218\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from IPython.core.display import display\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import  svm, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κατηγοριοποίηση δεδομένων κειμένου από ειδησιογραφικά άρθρα 5 κατηγοριών:</br>\n",
    "Classifying text data from articles of 5 different categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['business', 'entertainment', 'politics', 'sport', 'tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get names of files for testing and training\n",
    "Data set consists of 2225 documents from a news website\n",
    "corresponding to stories in five topical areas from 2004-2005.</br>\n",
    "</br>\n",
    "80% of data points (files) will be used for training, the remaining 20% will be used for testing.</br>\n",
    "We will be collecting the files names as elements in two lists (one for each purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files_path = 'fulltext/data/'\n",
    "train_files = []    # list of files names that will be used for training\n",
    "test_files = []     # list of files names that will be used for testing\n",
    "\n",
    "for category in categories:\n",
    "    # get all txt files names from current category\n",
    "    files = glob.glob(files_path+category+'/*.txt')\n",
    "    # sort them alphabetically\n",
    "    files.sort()\n",
    "    # separate list so that first 80% will be copied into the train_files list\n",
    "    sep_index = round(len(files) * 0.8)\n",
    "    train_files.extend(files[:sep_index])\n",
    "    test_files.extend(files[sep_index:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total files: 2225\")\n",
    "print(\"# of train files: \"+str(len(train_files)))\n",
    "print(\"# of test files: \"+str(len(test_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create file train_set.tsv\n",
    "Columns: id, title, content, category <br>\n",
    "_The id is the name of the text file with the first letter of the category prepended (ex. \"b001\")_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('train_set.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    # write header row\n",
    "    tsv_writer.writerow(['id', 'title', 'content', 'category'])\n",
    "    # write rows for all other files\n",
    "    for file_path in train_files:\n",
    "        with open(file_path) as f:\n",
    "            # read all lines of file split them in a list removing '\\n'\n",
    "            # iterate through elements in list removing empty ones (empty strings are False)\n",
    "            try:\n",
    "                content = [line for line in f.read().splitlines() if line.strip()]\n",
    "                # unpack the information we need from the file path\n",
    "                _, _, cat, name, _ = file_path.replace('.', '/').split('/')\n",
    "                # Write row\n",
    "                tsv_writer.writerow([cat[0]+name,\n",
    "                                 content.pop(0),\n",
    "                                 \" \".join(content),\n",
    "                                 cat])\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError for file: \"+file_path+\". File skipped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create file test_set.tsv\n",
    "Columns: id, title, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('test_set.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    # write header row\n",
    "    tsv_writer.writerow(['id', 'title', 'content'])\n",
    "    # write rows for all other files\n",
    "    for file_path in test_files:\n",
    "        with open(file_path) as f:\n",
    "            # read all lines of file split them in a list removing '\\n'\n",
    "            # iterate through elements in list removing empty ones (empty strings are False)\n",
    "            try:\n",
    "                content = [line for line in f.read().splitlines() if line.strip()]\n",
    "                # unpack the information we need from the file path\n",
    "                _, _, cat, name, _ = file_path.replace('.', '/').split('/')\n",
    "                # Write row\n",
    "                tsv_writer.writerow([cat[0]+name,\n",
    "                                 content.pop(0),\n",
    "                                 \" \".join(content)])\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError for file: \"+file_path+\". File skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create a file with the full dataset: dataset.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('dataset.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    # write header row\n",
    "    tsv_writer.writerow(['id', 'title', 'content', 'category'])\n",
    "    # write rows for all other files\n",
    "    for file_path in chain(train_files, test_files):\n",
    "        with open(file_path) as f:\n",
    "            # read all lines of file split them in a list removing '\\n'\n",
    "            # iterate through elements in list removing empty ones (empty strings are False)\n",
    "            try:\n",
    "                content = [line for line in f.read().splitlines() if line.strip()]\n",
    "                # unpack the information we need from the file path\n",
    "                _, _, cat, name, _ = file_path.replace('.', '/').split('/')\n",
    "                # Write row\n",
    "                tsv_writer.writerow([cat[0]+name,\n",
    "                                 content.pop(0),\n",
    "                                 \" \".join(content),\n",
    "                                 cat])\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError for file: \"+file_path+\". File skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a DataFrame for the data_set (id column as the index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#testdf = pd.read_csv(\"test_set.tsv\", sep='\\t', index_col='id')\n",
    "#traindf = pd.read_csv(\"train_set.tsv\", sep='\\t', index_col='id')\n",
    "datadf = pd.read_csv(\"dataset.tsv\", sep='\\t', index_col='id')\n",
    "datadf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Δημιουργία WordCloud\n",
    "**Create a WordCloud for the articles of each category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create our own stopWord list:\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(['say', 'said', 'saying', 'will', 'many', 'new', 'people', 'now', 'one'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#create wordcloud\n",
    "# select rows where the id contains 'b' (=business) using filter\n",
    "wordcloud_business = WordCloud(\n",
    "    width = 800,\n",
    "    height = 800,\n",
    "    background_color = 'black',\n",
    "    stopwords = stopwords).generate(\" \".join(title+' '+content\n",
    "                                             for title, content in datadf.filter(like='b', axis=0)\n",
    "                                                                   [['title', 'content']].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "business_image = wordcloud_business.to_image()\n",
    "display(business_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "wordcloud_entertainment = WordCloud(\n",
    "    width = 800,\n",
    "    height = 800,\n",
    "    background_color = 'black',\n",
    "    stopwords = stopwords).generate(str(datadf.filter(like='e', axis=0)[['title', 'content']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#show it\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud_entertainment)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud_politics = WordCloud(\n",
    "    width = 800,\n",
    "    height = 800,\n",
    "    background_color = 'black',\n",
    "    stopwords = stopwords).generate(str(datadf.filter(like='p', axis=0)[['title', 'content']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#show it\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud_politics)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud_sport = WordCloud(\n",
    "    width = 800,\n",
    "    height = 800,\n",
    "    background_color = 'black',\n",
    "    stopwords = stopwords).generate(str(datadf.filter(like='s', axis=0)[['title', 'content']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#show it\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud_sport)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud_tech = WordCloud(\n",
    "    width = 800,\n",
    "    height = 800,\n",
    "    background_color = 'black',\n",
    "    stopwords = stopwords).generate(str(datadf.filter(like='t', axis=0)[['title', 'content']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#show it\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud_tech)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 Υλοποίηση Κατηγοριοποίησης (Classification)\n",
    "**Data Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Cleaning and Pre-processing the data\n",
    "Create a Pandas Series, adding it as a new row into the datadf,\n",
    "by concatenating the title and content column of the datadf <br>\n",
    "Clean up text:<br>\n",
    "- Add a space before performing the sum to not connect words together accidentally\n",
    "- Make all the words lower case to facilitate clean up, using `.lower`\n",
    "- Remove our list of stopwords\n",
    "- Remove punctuation and special characters using `re.sub`\n",
    "- Remove all words containing digits, and any digits using `re.sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datadf['text'] = datadf[['title', 'content']]\\\n",
    "    .apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\\\n",
    "    .apply(lambda item: list(filter(lambda word: word not in stopwords, item.lower().split())))\\\n",
    "    .apply(lambda item: re.sub('[^A-Za-z0-9 ]+', '', ' '.join(item)))\\\n",
    "    .apply(lambda item: re.sub(r'\\w*\\d\\w*', '', item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words\n",
    "Create bag-of-words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_features=3000, stop_words='english')\n",
    "\n",
    "bow_X = bow_vectorizer.fit_transform(datadf.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(bow_X[0:1].T.todense(), index=bow_vectorizer.get_feature_names(), columns=[\"counts\"])\\\n",
    ".sort_values(by=[\"counts\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer= TfidfVectorizer(ngram_range=(1,2), max_features=3000, stop_words='english')\n",
    "\n",
    "tfidf_X = tfidf_vectorizer.fit_transform(datadf.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf_X[0:1].T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\\\n",
    ".sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate data into train (80%) and test (20%) set <br>\n",
    "Use the stratify parameter to ensure that the split between the different categories is done equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_X, datadf.category,\n",
    "                                                                test_size=0.2, stratify=datadf.category)\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_X, datadf.category,\n",
    "                                                     test_size=0.2, stratify=datadf.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### B) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#instantiate the model\n",
    "svm_clf = svm.SVC()\n",
    "\n",
    "# train the model on the BoW training set\n",
    "svm_clf.fit(X_train_bow, y_train_bow)\n",
    "# predict the BoW test set\n",
    "y_pred_svm_bow = svm_clf.predict(X_test_bow)\n",
    "\n",
    "# train the model on the TF/IDF training set (previous weights and variables are reset)\n",
    "svm_clf.fit(X_train_tfidf, y_train_tfidf)\n",
    "# predict the TF/IDF test set\n",
    "y_pred_svm_tfidf = svm_clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the BoW training set\n",
    "rf.fit(X_train_bow, y_train_bow)\n",
    "# predict the BoW test set\n",
    "y_pred_rf_bow = rf.predict(X_test_bow)\n",
    "\n",
    "# train the model on the TF/IDF training set (previous weights and variables are reset)\n",
    "rf.fit(X_train_tfidf, y_train_tfidf)\n",
    "# predict the TF/IDF test set\n",
    "y_pred_rf_tfidf = rf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the model on the BoW training set\n",
    "nb.fit(X_train_bow.toarray(), y_train_bow)\n",
    "# predict the BoW test set\n",
    "y_pred_nb_bow = nb.predict(X_test_bow.toarray())\n",
    "\n",
    "# Train the model on the TF/IDF training set (previous weights and variables are reset)\n",
    "nb.fit(X_train_tfidf.toarray(), y_train_tfidf)\n",
    "# predict the TF/IDF test set\n",
    "y_pred_nb_tfidf = nb.predict(X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Precision: what percentage was classified correctly?\n",
    "print(\"Precision SVM for BoW:\",metrics.precision_score(y_test_bow, y_pred_svm_bow, average=None))\n",
    "print(\"Precision SVM for TF/IDF:\",metrics.precision_score(y_test_tfidf, y_pred_svm_tfidf, average=None))\n",
    "print(\"Precision RF for BoW:\",metrics.precision_score(y_test_bow, y_pred_rf_bow, average=None))\n",
    "print(\"Precision RF for TF/IDF:\",metrics.precision_score(y_test_tfidf, y_pred_rf_tfidf, average=None))\n",
    "print(\"Precision NB for BoW:\",metrics.precision_score(y_test_bow, y_pred_nb_bow, average=None))\n",
    "print(\"Precision NB for TF/IDF:\",metrics.precision_score(y_test_tfidf, y_pred_nb_tfidf, average=None))\n",
    "\n",
    "# Model Recall\n",
    "print(\"Recall SVM for BoW:\",metrics.recall_score(y_test_bow, y_pred_svm_bow, average=None))\n",
    "print(\"Recall SVM for TF/IDF:\",metrics.recall_score(y_test_tfidf, y_pred_svm_tfidf, average=None))\n",
    "print(\"Recall RF for BoW:\",metrics.recall_score(y_test_bow, y_pred_rf_bow, average=None))\n",
    "print(\"Recall RF for TF/IDF:\",metrics.recall_score(y_test_tfidf, y_pred_rf_tfidf, average=None))\n",
    "print(\"Recall NB for BoW:\",metrics.recall_score(y_test_bow, y_pred_nb_bow, average=None))\n",
    "print(\"Recall NB for TF/IDF:\",metrics.recall_score(y_test_tfidf, y_pred_nb_tfidf, average=None))\n",
    "\n",
    "# F-Measure\n",
    "print(\"F-Measure SVM for BoW:\", metrics.f1_score(y_test_bow, y_pred_svm_bow, average=None))\n",
    "print(\"F-Measure SVM for TF/IDF:\", metrics.f1_score(y_test_tfidf, y_pred_svm_tfidf, average=None))\n",
    "print(\"F-Measure RF for BoW:\", metrics.f1_score(y_test_bow, y_pred_rf_bow, average=None))\n",
    "print(\"F-Measure RF for TF/IDF:\", metrics.f1_score(y_test_tfidf, y_pred_rf_tfidf, average=None))\n",
    "print(\"F-Measure NB for BoW:\", metrics.f1_score(y_test_bow, y_pred_nb_bow, average=None))\n",
    "print(\"F-Measure NB for TF/IDF:\", metrics.f1_score(y_test_tfidf, y_pred_nb_tfidf, average=None))\n",
    "\n",
    "print()\n",
    "# compare actual response values (y_test) with predicted response values (y_pred)\n",
    "print(\"Accuracy SVM for BoW:\",metrics.accuracy_score(y_test_bow, y_pred_svm_bow))\n",
    "print(\"Accuracy SVM for TF/IDF:\",metrics.accuracy_score(y_test_tfidf, y_pred_svm_tfidf))\n",
    "print(\"Accuracy RF for BoW:\",metrics.accuracy_score(y_test_bow, y_pred_rf_bow))\n",
    "print(\"Accuracy RF for TF/IDF:\",metrics.accuracy_score(y_test_tfidf, y_pred_rf_tfidf))\n",
    "print(\"Accuracy NB for BoW:\",metrics.accuracy_score(y_test_bow, y_pred_nb_bow))\n",
    "print(\"Accuracy NB for TF/IDF:\",metrics.accuracy_score(y_test_tfidf, y_pred_nb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cross_val_score(svm_clf, X_train_tfidf, y_train_tfidf, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(svm_clf, X_train_tfidf, y_train_tfidf, cv=10, scoring='precision_macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}